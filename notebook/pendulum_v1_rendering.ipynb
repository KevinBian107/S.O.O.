{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Pendulum Control\n",
    "Zero reward is the best condition for the pendulum control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from mvp.env_pendulum import PendulumEnv\n",
    "\n",
    "from itertools import count\n",
    "import torch\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from gym.wrappers import NormalizeObservation, TransformObservation, NormalizeReward, TransformReward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum Control Gym\n",
    "\n",
    "## Description\n",
    "The inverted pendulum swingup problem is a classic problem in control theory. The system consists of a pendulum attached at one end to a fixed point, with the other end being free. The pendulum starts in a random position, and the goal is to apply torque to the free end to swing it into an upright position, where its center of gravity is right above the fixed point.\n",
    "\n",
    "### Pendulum Coordinate System\n",
    "\n",
    "- **x-y**: Cartesian coordinates of the pendulum’s end in meters.\n",
    "- **theta**: Angle in radians.\n",
    "- **tau**: Torque in N·m, defined as positive counter-clockwise.\n",
    "\n",
    "### Action Space\n",
    "The action is an ndarray with shape `(1,)` representing the torque applied to the free end of the pendulum.\n",
    "\n",
    "| Num | Action | Min | Max |\n",
    "|-----|--------|-----|-----|\n",
    "| 0   | Torque | -2.0| 2.0 |\n",
    "\n",
    "### Observation Space\n",
    "The observation is an ndarray with shape `(3,)` representing the x-y coordinates of the pendulum’s free end and its angular velocity.\n",
    "\n",
    "| Num | Observation      | Min | Max |\n",
    "|-----|------------------|-----|-----|\n",
    "| 0   | x = cos(theta)   | -1.0| 1.0 |\n",
    "| 1   | y = sin(theta)   | -1.0| 1.0 |\n",
    "| 2   | Angular Velocity | -8.0| 8.0 |\n",
    "\n",
    "### Rewards\n",
    "The reward function is defined as:\n",
    "\n",
    "$r = -(theta^2 + 0.1 * theta_dt^2 + 0.001 * torque^2)$\n",
    "\n",
    "where `theta` is the pendulum’s angle normalized between `[-pi, pi]` (with 0 being in the upright position). The minimum reward that can be obtained is `-(pi^2 + 0.1 * 8^2 + 0.001 * 2^2) = -16.2736044`, while the maximum reward is zero (pendulum is upright with zero velocity and no torque applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='Pendulum-v4',\n",
    "    entry_point='mvp.env_pendulum:PendulumEnv',\n",
    "    max_episode_steps=1000)\n",
    "env = gym.make('Pendulum-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Model\n",
    "\n",
    "- KL Divergence on Policy (probabilistic distribution of actions outputted by our model) is very important\n",
    "- Seems to be stack, may need good experience to do good control\n",
    "- Swinging up is harder and takes longer for pure PPO\n",
    "- Normalization causes weird behavior, need to figure out why\n",
    "- When stuck at bottom, seems to be very hard to accomplish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvp.ppo_continuous import ActorCritic\n",
    "\n",
    "path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"ppo_vector.pth\")\n",
    "\n",
    "env = PendulumEnv(render_mode=\"human\")\n",
    "n_actions = env.action_space.shape[0]\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "num_eval_episodes = 10\n",
    "\n",
    "# model = Agent(n_observations, n_actions).to(device)\n",
    "model = ActorCritic(env)\n",
    "model.load_state_dict(torch.load(path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "for i_episode in range(num_eval_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        env.render()\n",
    "        # Get action from the model (assuming it outputs action_mean, action_std, and value)\n",
    "        with torch.no_grad():\n",
    "            action_mean, _, _ = model(state)\n",
    "        \n",
    "        # Take the mean action (no sampling here for deterministic behavior)\n",
    "        action = action_mean.cpu().numpy()[0]\n",
    "        print(action)\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode finished after {t+1} timesteps\")\n",
    "            break\n",
    "\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FMPPO\n",
    "\n",
    "Some how just constantly swings around, but does show fast convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvp.fmppo_continuous import ActorCriticUPN\n",
    "\n",
    "path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"pendulum_fmppo.pth\")\n",
    "\n",
    "env = PendulumEnv(render_mode=\"human\")\n",
    "n_actions = env.action_space.shape[0]\n",
    "state, _ = env.reset()\n",
    "n_observations = len(state)\n",
    "num_eval_episodes = 10\n",
    "\n",
    "# Create and load the model\n",
    "model = ActorCriticUPN(n_observations, n_actions).to(device)\n",
    "model.load_state_dict(torch.load(path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "for i_episode in range(num_eval_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    for t in count():\n",
    "        env.render()\n",
    "        # Get action from the model\n",
    "        with torch.no_grad():\n",
    "            action_mean, _, _ = model(state)\n",
    "        \n",
    "        # Take the mean action (no sampling here for deterministic behavior)\n",
    "        action = action_mean.cpu().numpy()[0]\n",
    "        print(action)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode finished after {t+1} timesteps\")\n",
    "            break\n",
    "\n",
    "        state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvp.dqn_networks import DQN\n",
    "\n",
    "path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"pendulum_dqn_discrete_retrain.pth\")\n",
    "\n",
    "env = PendulumEnv(render_mode=\"human\")\n",
    "ACTION_MAP = np.linspace(-2, 2, 5)  # 5 actions ranging from -2 to 2\n",
    "n_actions = len(ACTION_MAP)\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "model = DQN(n_observations, n_actions).to(device)\n",
    "model.load_state_dict(torch.load(path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "num_eval_episodes = 10\n",
    "for i_episode in range(num_eval_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        env.render()\n",
    "\n",
    "        action_idx = model(state).max(1)[1]\n",
    "        actual_action = ACTION_MAP[action_idx.item()]\n",
    "        print(action)\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step([actual_action])\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode finished after {t+1} timesteps\")\n",
    "            break\n",
    "\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Post-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvp.ppo_continuous import ActorCritic  # PPO Model\n",
    "from mvp.fmppo_continuous import ActorCriticUPN  # FM-PPO Model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters for evaluation\n",
    "num_eval_episodes = 500  # Number of episodes for evaluation\n",
    "max_timesteps = 200  # Max timesteps per episode\n",
    "\n",
    "# Function to evaluate a model and collect rewards\n",
    "def evaluate_model(model, env, num_eval_episodes, max_timesteps):\n",
    "    all_rewards = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for i_episode in range(num_eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        episode_rewards = 0\n",
    "\n",
    "        for t in range(max_timesteps):\n",
    "            with torch.no_grad():\n",
    "                action_mean, _, _ = model(state)\n",
    "\n",
    "            # Take the mean action (no sampling for deterministic behavior)\n",
    "            action = action_mean.cpu().numpy()[0]\n",
    "\n",
    "            # Step the environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_rewards += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "\n",
    "        all_rewards.append(episode_rewards)\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "# Load the PPO model\n",
    "ppo_model_path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"ppo_ITER_3000_KL_0.01_RUN_1.pth\")\n",
    "ppo_model = ActorCritic(n_states=len(env.reset()[0]), n_actions=env.action_space.shape[0]).to(device)\n",
    "ppo_model.load_state_dict(torch.load(ppo_model_path, map_location=device))\n",
    "\n",
    "# Load the FM-PPO model (with UPN)\n",
    "fmppo_model_path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"pendulum_fmppo.pth\")\n",
    "fmppo_model = ActorCriticUPN(n_states=len(env.reset()[0]), n_actions=env.action_space.shape[0]).to(device)\n",
    "fmppo_model.load_state_dict(torch.load(fmppo_model_path, map_location=device))\n",
    "\n",
    "# Set up the environment (without rendering)\n",
    "env = PendulumEnv(render_mode=None)\n",
    "\n",
    "# Evaluate PPO Model\n",
    "ppo_rewards = evaluate_model(ppo_model, env, num_eval_episodes, max_timesteps)\n",
    "\n",
    "# Evaluate FM-PPO Model (UPN-based)\n",
    "fmppo_rewards = evaluate_model(fmppo_model, env, num_eval_episodes, max_timesteps)\n",
    "\n",
    "# Calculate average rewards for both models\n",
    "average_ppo_reward = sum(ppo_rewards) / num_eval_episodes\n",
    "average_fmppo_reward = sum(fmppo_rewards) / num_eval_episodes\n",
    "\n",
    "# Plot the rewards for both models\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_eval_episodes + 1), ppo_rewards, label=\"PPO Episode Reward\")\n",
    "plt.plot(range(1, num_eval_episodes + 1), fmppo_rewards, label=\"FM-PPO Episode Reward\")\n",
    "plt.axhline(y=average_ppo_reward, color='r', linestyle='--', label=f'PPO Avg Reward = {average_ppo_reward:.2f}')\n",
    "plt.axhline(y=average_fmppo_reward, color='g', linestyle='--', label=f'FM-PPO Avg Reward = {average_fmppo_reward:.2f}')\n",
    "plt.title(\"PPO vs FM-PPO Episode Rewards over Multiple Evaluations\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for evaluation\n",
    "num_eval_episodes = 500  # Number of episodes for evaluation\n",
    "max_timesteps = 200  # Max timesteps per episode\n",
    "eval_interval = 50  # Number of episodes to average over\n",
    "\n",
    "# Function to evaluate a model and collect rewards\n",
    "def evaluate_model(model, env, num_eval_episodes, max_timesteps, eval_interval):\n",
    "    all_rewards = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for i_episode in range(num_eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        episode_rewards = 0\n",
    "\n",
    "        for t in range(max_timesteps):\n",
    "            with torch.no_grad():\n",
    "                action_mean, _, _ = model(state)\n",
    "\n",
    "            # Take the mean action (no sampling for deterministic behavior)\n",
    "            action = action_mean.cpu().numpy()[0]\n",
    "\n",
    "            # Step the environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_rewards += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "\n",
    "        all_rewards.append(episode_rewards)\n",
    "\n",
    "    # Calculate average rewards for every 'eval_interval' episodes\n",
    "    avg_rewards = [sum(all_rewards[i:i+eval_interval]) / eval_interval for i in range(0, num_eval_episodes, eval_interval)]\n",
    "\n",
    "    return avg_rewards\n",
    "\n",
    "# Set up the environment (without rendering)\n",
    "env = PendulumEnv(render_mode=None)\n",
    "\n",
    "# Load the PPO model\n",
    "ppo_model_path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"ppo_ITER_300_KL_0.01_RUN_1.pth\")\n",
    "ppo_model = ActorCritic(n_states=len(env.reset()[0]), n_actions=env.action_space.shape[0]).to(device)\n",
    "ppo_model.load_state_dict(torch.load(ppo_model_path, map_location=device))\n",
    "\n",
    "# Load the FM-PPO model (with UPN)\n",
    "fmppo_model_path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"pendulum_fmppo.pth\")\n",
    "fmppo_model = ActorCriticUPN(n_states=len(env.reset()[0]), n_actions=env.action_space.shape[0]).to(device)\n",
    "fmppo_model.load_state_dict(torch.load(fmppo_model_path, map_location=device))\n",
    "\n",
    "# Evaluate PPO Model\n",
    "ppo_avg_rewards = evaluate_model(ppo_model, env, num_eval_episodes, max_timesteps, eval_interval)\n",
    "\n",
    "# Evaluate FM-PPO Model (UPN-based)\n",
    "fmppo_avg_rewards = evaluate_model(fmppo_model, env, num_eval_episodes, max_timesteps, eval_interval)\n",
    "\n",
    "# Generate x-axis labels representing each 50-episode interval\n",
    "x_labels = range(1, num_eval_episodes // eval_interval + 1)\n",
    "\n",
    "# Plot the average rewards for both models\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_labels, ppo_avg_rewards, label=\"PPO Avg Reward per 50 Episodes\", marker='o')\n",
    "plt.plot(x_labels, fmppo_avg_rewards, label=\"FM-PPO Avg Reward per 50 Episodes\", marker='o')\n",
    "plt.title(\"PPO vs FM-PPO Average Rewards over 50 Episode Intervals\")\n",
    "plt.xlabel(\"Episode Interval (50 episodes each)\")\n",
    "plt.ylabel(\"Average Total Reward\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
