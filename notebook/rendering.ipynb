{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Pendulum Control\n",
    "Zero reward is the best condition for the pendulum control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from mvp.env_pendulum import PendulumEnv\n",
    "\n",
    "from itertools import count\n",
    "import torch\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum Control Gym\n",
    "\n",
    "## Description\n",
    "The inverted pendulum swingup problem is a classic problem in control theory. The system consists of a pendulum attached at one end to a fixed point, with the other end being free. The pendulum starts in a random position, and the goal is to apply torque to the free end to swing it into an upright position, where its center of gravity is right above the fixed point.\n",
    "\n",
    "### Pendulum Coordinate System\n",
    "\n",
    "- **x-y**: Cartesian coordinates of the pendulum’s end in meters.\n",
    "- **theta**: Angle in radians.\n",
    "- **tau**: Torque in N·m, defined as positive counter-clockwise.\n",
    "\n",
    "### Action Space\n",
    "The action is an ndarray with shape `(1,)` representing the torque applied to the free end of the pendulum.\n",
    "\n",
    "| Num | Action | Min | Max |\n",
    "|-----|--------|-----|-----|\n",
    "| 0   | Torque | -2.0| 2.0 |\n",
    "\n",
    "### Observation Space\n",
    "The observation is an ndarray with shape `(3,)` representing the x-y coordinates of the pendulum’s free end and its angular velocity.\n",
    "\n",
    "| Num | Observation      | Min | Max |\n",
    "|-----|------------------|-----|-----|\n",
    "| 0   | x = cos(theta)   | -1.0| 1.0 |\n",
    "| 1   | y = sin(theta)   | -1.0| 1.0 |\n",
    "| 2   | Angular Velocity | -8.0| 8.0 |\n",
    "\n",
    "### Rewards\n",
    "The reward function is defined as:\n",
    "\n",
    "$r = -(theta^2 + 0.1 * theta_dt^2 + 0.001 * torque^2)$\n",
    "\n",
    "where `theta` is the pendulum’s angle normalized between `[-pi, pi]` (with 0 being in the upright position). The minimum reward that can be obtained is `-(pi^2 + 0.1 * 8^2 + 0.001 * 2^2) = -16.2736044`, while the maximum reward is zero (pendulum is upright with zero velocity and no torque applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinb/anaconda3/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Pendulum-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "register(\n",
    "    id='Pendulum-v0',\n",
    "    entry_point='mvp.env_pendulum:PendulumEnv',\n",
    "    max_episode_steps=1000)\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-2.0, 2.0, (1,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvp.ppo_continuous import ActorCritic\n",
    "\n",
    "path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"pendulum_ppo_continuous.pth\")\n",
    "\n",
    "env = PendulumEnv(render_mode=\"human\")\n",
    "n_actions = env.action_space.shape[0]\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "num_eval_episodes = 10\n",
    "\n",
    "model = ActorCritic(n_observations, n_actions).to(device)\n",
    "model.load_state_dict(torch.load(path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "for i_episode in range(num_eval_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        env.render()\n",
    "        # Get action from the model (assuming it outputs action_mean, action_std, and value)\n",
    "        with torch.no_grad():\n",
    "            action_mean, _, _ = model(state)\n",
    "        \n",
    "        # Take the mean action (no sampling here for deterministic behavior)\n",
    "        action = action_mean.cpu().numpy()[0]\n",
    "        print(action)\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode finished after {t+1} timesteps\")\n",
    "            break\n",
    "\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.39533386]\n",
      "[-0.40201777]\n",
      "[-0.4065948]\n",
      "[-0.41026467]\n",
      "[-0.41303012]\n",
      "[-0.41503626]\n",
      "[-0.41654584]\n",
      "[-0.4176262]\n",
      "[-0.4180265]\n",
      "[-0.4174115]\n",
      "[-0.41591188]\n",
      "[-0.41306525]\n",
      "[-0.40932482]\n",
      "[-0.403414]\n",
      "[-0.40000105]\n",
      "[-0.39917445]\n",
      "[-0.39850575]\n",
      "[-0.39685243]\n",
      "[-0.3957921]\n",
      "[-0.39484376]\n",
      "[-0.3931168]\n",
      "[-0.39055592]\n",
      "[-0.3874311]\n",
      "[-0.38381696]\n",
      "[-0.37985164]\n",
      "[-0.375094]\n",
      "[-0.37066114]\n",
      "[-0.3662364]\n",
      "[-0.36483496]\n",
      "[-0.36597803]\n",
      "[-0.3687826]\n",
      "[-0.37680963]\n",
      "[-0.38630557]\n",
      "[-0.3957914]\n",
      "[-0.40228394]\n",
      "[-0.4068883]\n",
      "[-0.41052338]\n",
      "[-0.41326147]\n",
      "[-0.41525012]\n",
      "[-0.4167564]\n",
      "[-0.41786176]\n",
      "[-0.41831136]\n",
      "[-0.41770178]\n",
      "[-0.41623068]\n",
      "[-0.4133582]\n",
      "[-0.40954986]\n",
      "[-0.4036768]\n",
      "[-0.40041083]\n",
      "[-0.39961144]\n",
      "[-0.39888108]\n",
      "[-0.3972978]\n",
      "[-0.39629805]\n",
      "[-0.3954124]\n",
      "[-0.39342123]\n",
      "[-0.3907783]\n",
      "[-0.38769904]\n",
      "[-0.38402343]\n",
      "[-0.38003206]\n",
      "[-0.37526643]\n",
      "[-0.3706262]\n",
      "[-0.36609763]\n",
      "[-0.36444962]\n",
      "[-0.36566842]\n",
      "[-0.36854452]\n",
      "[-0.3767456]\n",
      "[-0.38644177]\n",
      "[-0.39620495]\n",
      "[-0.40244985]\n",
      "[-0.4070899]\n",
      "[-0.41070265]\n",
      "[-0.4134178]\n",
      "[-0.41540846]\n",
      "[-0.41694075]\n",
      "[-0.4180627]\n",
      "[-0.4185775]\n",
      "[-0.4180223]\n",
      "[-0.41657117]\n",
      "[-0.41370645]\n",
      "[-0.40983206]\n",
      "[-0.40401012]\n",
      "[-0.40082538]\n",
      "[-0.4000635]\n",
      "[-0.39927354]\n",
      "[-0.3977591]\n",
      "[-0.39681304]\n",
      "[-0.39585203]\n",
      "[-0.39374638]\n",
      "[-0.39103097]\n",
      "[-0.3879273]\n",
      "[-0.38427752]\n",
      "[-0.38027316]\n",
      "[-0.3755207]\n",
      "[-0.37066305]\n",
      "[-0.36601797]\n",
      "[-0.3640638]\n",
      "[-0.36532852]\n",
      "[-0.3682502]\n",
      "[-0.37651378]\n",
      "[-0.38640833]\n",
      "[-0.3962375]\n",
      "[-0.4025195]\n",
      "[-0.40720367]\n",
      "[-0.41080433]\n",
      "[-0.4134966]\n",
      "[-0.4155116]\n",
      "[-0.41707757]\n",
      "[-0.41822642]\n",
      "[-0.41882178]\n",
      "[-0.41836217]\n",
      "[-0.41693163]\n",
      "[-0.41411263]\n",
      "[-0.41017592]\n",
      "[-0.40441376]\n",
      "[-0.40124655]\n",
      "[-0.4005158]\n",
      "[-0.39968783]\n",
      "[-0.39824107]\n",
      "[-0.39734197]\n",
      "[-0.3962835]\n",
      "[-0.39409816]\n",
      "[-0.39131892]\n",
      "[-0.3881733]\n",
      "[-0.3845827]\n",
      "[-0.38057777]\n",
      "[-0.37586045]\n",
      "[-0.37079012]\n",
      "[-0.36602896]\n",
      "[-0.36367655]\n",
      "[-0.36495537]\n",
      "[-0.3678946]\n",
      "[-0.3760965]\n",
      "[-0.38619217]\n",
      "[-0.39613187]\n",
      "[-0.40248728]\n",
      "[-0.40724814]\n",
      "[-0.41083026]\n",
      "[-0.41350955]\n",
      "[-0.4155589]\n",
      "[-0.41716224]\n",
      "[-0.41834968]\n",
      "[-0.41904008]\n",
      "[-0.41870117]\n",
      "[-0.41730827]\n",
      "[-0.41457635]\n",
      "[-0.41059875]\n",
      "[-0.40489095]\n",
      "[-0.4016745]\n",
      "[-0.4009623]\n",
      "[-0.40012753]\n",
      "[-0.39875677]\n",
      "[-0.3978887]\n",
      "[-0.3967336]\n",
      "[-0.3944821]\n",
      "[-0.39164668]\n",
      "[-0.3886687]\n",
      "[-0.38516515]\n",
      "[-0.3809467]\n",
      "[-0.3762766]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m action \u001b[38;5;241m=\u001b[39m action_mean\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[0;32m---> 29\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode finished after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/CL/VNL-MVP/mvp/env_pendulum.py:139\u001b[0m, in \u001b[0;36mPendulumEnv.step\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([newth, newthdot])\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs(), \u001b[38;5;241m-\u001b[39mcosts, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/Desktop/CL/VNL-MVP/mvp/env_pendulum.py:254\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    253\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# mode == \"rgb_array\":\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mvp.fmppo_continuous import ActorCriticUPN\n",
    "\n",
    "path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"pendulum_ppo_upn_continuous.pth\")\n",
    "\n",
    "env = PendulumEnv(render_mode=\"human\")\n",
    "n_actions = env.action_space.shape[0]\n",
    "state, _ = env.reset()\n",
    "n_observations = len(state)\n",
    "num_eval_episodes = 10\n",
    "\n",
    "# Create and load the model\n",
    "model = ActorCriticUPN(n_observations, n_actions).to(device)\n",
    "model.load_state_dict(torch.load(path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "for i_episode in range(num_eval_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    for t in count():\n",
    "        env.render()\n",
    "        # Get action from the model\n",
    "        with torch.no_grad():\n",
    "            action_mean, _, _ = model(state)\n",
    "        \n",
    "        # Take the mean action (no sampling here for deterministic behavior)\n",
    "        action = action_mean.cpu().numpy()[0]\n",
    "        print(action)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode finished after {t+1} timesteps\")\n",
    "            break\n",
    "\n",
    "        state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvp.dqn_networks import DQN\n",
    "\n",
    "path = os.path.join(os.getcwd(), \"..\", \"mvp\", \"params\", \"pendulum_dqn_discrete_retrain.pth\")\n",
    "\n",
    "env = PendulumEnv(render_mode=\"human\")\n",
    "ACTION_MAP = np.linspace(-2, 2, 5)  # 5 actions ranging from -2 to 2\n",
    "n_actions = len(ACTION_MAP)\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "model = DQN(n_observations, n_actions).to(device)\n",
    "model.load_state_dict(torch.load(path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "num_eval_episodes = 10\n",
    "for i_episode in range(num_eval_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        env.render()\n",
    "\n",
    "        action_idx = model(state).max(1)[1]\n",
    "        actual_action = ACTION_MAP[action_idx.item()]\n",
    "        print(action)\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step([actual_action])\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode finished after {t+1} timesteps\")\n",
    "            break\n",
    "\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
