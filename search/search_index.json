{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sub-Optimality Optimization (S.O.O) \u00b6 To me, I feel like the traditional perspective of continual learning has always been deem as the process of doing 2 things: \"how to preerve more when learning a new task (memory stability)\" and \"how to even learn under this condition (memory plasticity)\". However, I would like to deem the problem of CL from a different perspective. I would define the problem of CL as: Learning the best action to take given that we are in a specific condition. Each tasks is sort of chained together to form a harder problem that we are trying to solve. With this formulation, we are breaking the problem into two parts (goes with the overall workflow of EM algorithm) where one part (VAE) focuses on creating a cohesive picture of various environment (creating the picture of the \"bigger\" environment) and the other (Model-free RL) searches given a niche projection, or an instance, of the \"bigger\" environment. VAE as construction : How to learn a correct representation of the environment? What are the low dimensional key feature of each environment that we can learn? How should we represent these features that we learned about the enviornment? Latent? Distribution? RL as Search : Given the features of the environment, how can we build an search algorithm that finds the optimal solution under the environmental projection and store them as a generalized idea (notice that this understanding is not discrete)? Maybe we need a highly parametrized network? This sort of ideas stems from thinking about how we learn, specifically how little kids would learn. Mathamatically, it is an idea that includes 3 different domains of RL: model-based, model-free, and RL as inference. Under this perspective, we are no longer dealing with different objective surfaces in different discrete number of tasks but rather looking at only one objective surface that gradually updates with more tasks involved (think of it as a \"bigger\" enviornment that encodes gradually) and the goal been finding a \"workable solution\" in such highly non-convex surface, or we can say optimization under the projection of each environment instance . We can even say that the search algorithm is sampling under such environmental model. Notebally, unlike traditional model-based RL where we try to model environment probability and does \"virtual imagination\", we try to learn an environment conditional distribution (do we want to represent it in this way? Essentially we are saying how likely the environment be, approximate with many Gaussian) for each search problem. The architecture of the networks may look something like this where we try to establish a understanding of the enviornmnet:","title":"Introduction"},{"location":"#sub-optimality-optimization-soo","text":"To me, I feel like the traditional perspective of continual learning has always been deem as the process of doing 2 things: \"how to preerve more when learning a new task (memory stability)\" and \"how to even learn under this condition (memory plasticity)\". However, I would like to deem the problem of CL from a different perspective. I would define the problem of CL as: Learning the best action to take given that we are in a specific condition. Each tasks is sort of chained together to form a harder problem that we are trying to solve. With this formulation, we are breaking the problem into two parts (goes with the overall workflow of EM algorithm) where one part (VAE) focuses on creating a cohesive picture of various environment (creating the picture of the \"bigger\" environment) and the other (Model-free RL) searches given a niche projection, or an instance, of the \"bigger\" environment. VAE as construction : How to learn a correct representation of the environment? What are the low dimensional key feature of each environment that we can learn? How should we represent these features that we learned about the enviornment? Latent? Distribution? RL as Search : Given the features of the environment, how can we build an search algorithm that finds the optimal solution under the environmental projection and store them as a generalized idea (notice that this understanding is not discrete)? Maybe we need a highly parametrized network? This sort of ideas stems from thinking about how we learn, specifically how little kids would learn. Mathamatically, it is an idea that includes 3 different domains of RL: model-based, model-free, and RL as inference. Under this perspective, we are no longer dealing with different objective surfaces in different discrete number of tasks but rather looking at only one objective surface that gradually updates with more tasks involved (think of it as a \"bigger\" enviornment that encodes gradually) and the goal been finding a \"workable solution\" in such highly non-convex surface, or we can say optimization under the projection of each environment instance . We can even say that the search algorithm is sampling under such environmental model. Notebally, unlike traditional model-based RL where we try to model environment probability and does \"virtual imagination\", we try to learn an environment conditional distribution (do we want to represent it in this way? Essentially we are saying how likely the environment be, approximate with many Gaussian) for each search problem. The architecture of the networks may look something like this where we try to establish a understanding of the enviornmnet:","title":"Sub-Optimality Optimization (S.O.O)"},{"location":"inspirations/","text":"Inspirations \u00b6 Biological \u00b6 The cerebellum have been long theorized to play an crucial rule in motor control and learning (Forward modeling). Corollary discharge encodes a efferent copy of the motor command to be processed to predict the consequences of actions before sensory feedback is available. Such process would help us predicts how the sensory state of our body will change and how should these actions be performed, achieving better performances in control. Wold Model (Environment Model) \u00b6 We believe that policy is to a task specific, but if we build an reward model or an world model , such model may be agonist of the environment or the task, achieveing continual learning purpose. The question becomes how can we build such representation? Through latent space and modifying architecture ? By modifying the training/learning algorithm and the fundamental way tha egnt learn things ? Philosophical (Constraint Solving) \u00b6 It is not about finding the best (optimal solution) each time, but rather finding a good enough solution (Optimal under our projection of the past trees that we have explored). Every agent that learns in different world serves as a constraint for each other. They explore the world from their own perspective and \u201cpull\u201d each other on the way to prevent any one of them from falling into the \u201clocal optimality illusion\u201d that one sees in one moment. Optimality in one instance may not be optimal in the long run, our optimization landscape continuously changes across time where the true surface is the surface that includes all tasks' surfaces. Gradually, the surface of this sequential optimizing task should reveal itself. It is a highly non convex optimization, all we can do is to believe the conservative tree that we built that has previous experience pulling on each other: frome ach sub-optimality we hope to achieve optimality. We can consider such model as a constrained process, forcing the agent to learn while incorporating its previous experiences or sampling under the expectation of previous world models . The forward model's facilitation does not come directly from action facilitation but rather from providing a better representation of the feature space \\(\\vec \\phi(\\vec x)\\) that the agent has built up. It's about operating in this imaginary state . It is never about explicitly facilitating actions but rather implicitly making the model understand the dynamics and interactions in this space more comprehensively.","title":"Inspirations"},{"location":"inspirations/#inspirations","text":"","title":"Inspirations"},{"location":"inspirations/#biological","text":"The cerebellum have been long theorized to play an crucial rule in motor control and learning (Forward modeling). Corollary discharge encodes a efferent copy of the motor command to be processed to predict the consequences of actions before sensory feedback is available. Such process would help us predicts how the sensory state of our body will change and how should these actions be performed, achieving better performances in control.","title":"Biological"},{"location":"inspirations/#wold-model-environment-model","text":"We believe that policy is to a task specific, but if we build an reward model or an world model , such model may be agonist of the environment or the task, achieveing continual learning purpose. The question becomes how can we build such representation? Through latent space and modifying architecture ? By modifying the training/learning algorithm and the fundamental way tha egnt learn things ?","title":"Wold Model (Environment Model)"},{"location":"inspirations/#philosophical-constraint-solving","text":"It is not about finding the best (optimal solution) each time, but rather finding a good enough solution (Optimal under our projection of the past trees that we have explored). Every agent that learns in different world serves as a constraint for each other. They explore the world from their own perspective and \u201cpull\u201d each other on the way to prevent any one of them from falling into the \u201clocal optimality illusion\u201d that one sees in one moment. Optimality in one instance may not be optimal in the long run, our optimization landscape continuously changes across time where the true surface is the surface that includes all tasks' surfaces. Gradually, the surface of this sequential optimizing task should reveal itself. It is a highly non convex optimization, all we can do is to believe the conservative tree that we built that has previous experience pulling on each other: frome ach sub-optimality we hope to achieve optimality. We can consider such model as a constrained process, forcing the agent to learn while incorporating its previous experiences or sampling under the expectation of previous world models . The forward model's facilitation does not come directly from action facilitation but rather from providing a better representation of the feature space \\(\\vec \\phi(\\vec x)\\) that the agent has built up. It's about operating in this imaginary state . It is never about explicitly facilitating actions but rather implicitly making the model understand the dynamics and interactions in this space more comprehensively.","title":"Philosophical (Constraint Solving)"},{"location":"next/","text":"Some Next Steps \u00b6 Literature Reviews & Mathamatical Formulations \u00b6 Need to do more literature reviews: How can we keep things in a neural network using perhaps transfer learning technqiues. How should we represent these features that we learned about the enviornment? Latent? Distribution? Need to revise the mathamatical formulation . Attempt to derive from MOMPO here More Questions \u00b6 There are 2 questions to solve (one easy and one hard): - The easy one is doing constraint optimization on model-free algorithm families. - The hard one is to reformulate this problem theoritically. Harder question to solve: Can I bake in the idea of having a world model into the algorithm itself, not just using networks. There is a theoritical perspective from EM, and then there is a practical perspective of how can we do it from constraint optimization . Here is a few question that I want to answer We need to change the fundamental constrained optimization's formulation from MOMPO. What if I say \\(q\\) distribution not as a action distribution (in MOMPO) but a latent distribution in the VAE (representing the model of the environment), then we add KL as a constraint on the VAE latent distribtion at each iteration, gradually constructing \\(q\\) . Can we still derive an ELBO for it (new MinMax Lagrangian duality optimization problem). Biologically related questions Does establishing a world model, similar to the Cerebellum's function, facilitate motor action execution by providing a motor plan derived from previous motor control experiences for additional guidance (compare to pure sensory feedback like in model-free RL)? Moreover, can this new motor learning process be incorporated into the GDP for future motor controls? See if such biologically inspired strategy (mechanistic insight) improves performance. See if the Forward Model would resemble functionality and behavior of the cerebellum (for example, showing gradual learning of new motor skills). With an change of the understanding for the rules of the world, can the algorithm still find a sub-optimal point in this training world such that it works still fine or even better than solely one-world-model trained agent in the other world? Code Base & Experiments: \u00b6 Implementation notes in here Adapt the motor control task of switching intention to the Atari game setting. Need to consider what phenomenon we would see when our idea actually works? Need a good testing paradigm. Easier and easier to learn new things? The model should develope an intuition of the enviornment and learn better when seeing similar environment? Learning a combined skill set from previous experiences of smaller skills that can be transferable should be easier? Being able to transfer back and have memory retention?","title":"Next Steps"},{"location":"next/#some-next-steps","text":"","title":"Some Next Steps"},{"location":"next/#literature-reviews-mathamatical-formulations","text":"Need to do more literature reviews: How can we keep things in a neural network using perhaps transfer learning technqiues. How should we represent these features that we learned about the enviornment? Latent? Distribution? Need to revise the mathamatical formulation . Attempt to derive from MOMPO here","title":"Literature Reviews &amp; Mathamatical Formulations"},{"location":"next/#more-questions","text":"There are 2 questions to solve (one easy and one hard): - The easy one is doing constraint optimization on model-free algorithm families. - The hard one is to reformulate this problem theoritically. Harder question to solve: Can I bake in the idea of having a world model into the algorithm itself, not just using networks. There is a theoritical perspective from EM, and then there is a practical perspective of how can we do it from constraint optimization . Here is a few question that I want to answer We need to change the fundamental constrained optimization's formulation from MOMPO. What if I say \\(q\\) distribution not as a action distribution (in MOMPO) but a latent distribution in the VAE (representing the model of the environment), then we add KL as a constraint on the VAE latent distribtion at each iteration, gradually constructing \\(q\\) . Can we still derive an ELBO for it (new MinMax Lagrangian duality optimization problem). Biologically related questions Does establishing a world model, similar to the Cerebellum's function, facilitate motor action execution by providing a motor plan derived from previous motor control experiences for additional guidance (compare to pure sensory feedback like in model-free RL)? Moreover, can this new motor learning process be incorporated into the GDP for future motor controls? See if such biologically inspired strategy (mechanistic insight) improves performance. See if the Forward Model would resemble functionality and behavior of the cerebellum (for example, showing gradual learning of new motor skills). With an change of the understanding for the rules of the world, can the algorithm still find a sub-optimal point in this training world such that it works still fine or even better than solely one-world-model trained agent in the other world?","title":"More Questions"},{"location":"next/#code-base-experiments","text":"Implementation notes in here Adapt the motor control task of switching intention to the Atari game setting. Need to consider what phenomenon we would see when our idea actually works? Need a good testing paradigm. Easier and easier to learn new things? The model should develope an intuition of the enviornment and learn better when seeing similar environment? Learning a combined skill set from previous experiences of smaller skills that can be transferable should be easier? Being able to transfer back and have memory retention?","title":"Code Base &amp; Experiments:"},{"location":"progress/","text":"SFM-PPO & SoFM-PPO Control Demos \u00b6 SFM-PPO is a vriant of PPO where an Supervised Forward Model (SFM) is added to understand the dynamics of teh environment amd SoFM-PPO is a variant of PPO (Sub-Optimal Forward Model) as well where instead of just the SFM, the model is also trained to find \"local minimum\" or sub-optimal minimum in one task in order for completions in more than just one task. The below are a few Deep-RL Half Cheetah agent demos: Your browser does not support the video tag. Trained using PPO on \"Jump Task\" and stuck on weird local minimum Your browser does not support the video tag. Trained using SFM-PPO on \"Normal Running Task\" Your browser does not support the video tag. Trained using SoFM-PPO on \"Normal Running Task\" but with world model intention bounding Latent Created Example \u00b6 Demonstartion of latent representation of agent during task Your browser does not support the video tag. Deep-RL SoFM-PPO Half Cheetah agent trained and visualize with PCA Performance in Sensory Delayed Environment \u00b6 Simple testing in mimicing delayed sensory environment like in real life. Trained with a transfered core. Deep-RL SoFM-PPO Half Cheetah agent evaluated on sensory delayed environment","title":"Preliminaries"},{"location":"progress/#sfm-ppo-sofm-ppo-control-demos","text":"SFM-PPO is a vriant of PPO where an Supervised Forward Model (SFM) is added to understand the dynamics of teh environment amd SoFM-PPO is a variant of PPO (Sub-Optimal Forward Model) as well where instead of just the SFM, the model is also trained to find \"local minimum\" or sub-optimal minimum in one task in order for completions in more than just one task. The below are a few Deep-RL Half Cheetah agent demos: Your browser does not support the video tag. Trained using PPO on \"Jump Task\" and stuck on weird local minimum Your browser does not support the video tag. Trained using SFM-PPO on \"Normal Running Task\" Your browser does not support the video tag. Trained using SoFM-PPO on \"Normal Running Task\" but with world model intention bounding","title":"SFM-PPO &amp; SoFM-PPO Control Demos"},{"location":"progress/#latent-created-example","text":"Demonstartion of latent representation of agent during task Your browser does not support the video tag. Deep-RL SoFM-PPO Half Cheetah agent trained and visualize with PCA","title":"Latent Created Example"},{"location":"progress/#performance-in-sensory-delayed-environment","text":"Simple testing in mimicing delayed sensory environment like in real life. Trained with a transfered core. Deep-RL SoFM-PPO Half Cheetah agent evaluated on sensory delayed environment","title":"Performance in Sensory Delayed Environment"}]}