{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Supervised & Sub-optimality Forward Model \u00b6 I am currently researching into the theoretical aspects of Continual Learning and Transfer Learning with advising from professor Talmo Pereira in the Salk Institute. I try to frame the general problem of Continual Learning from the perspective of Reinforcement Learning & Cognitive Neuroscience, hoping to develop algorithms that utilize the same strategies of \"how we learn\" onto an artificial agent. Moreover, I hope that such development can serve more than an engineering improvmenet, but rather contributing back to the neuroscience community to understand more about ourselves. I am developing a conceptual framework that may be used for families of algorithms and I am trying to build internal representations for artificial agents through designing a Forward Model , one similar to what we think the Cerabellum is doing in human brain, so their learned skills in one task can be modularized and transferable to other control tasks. The belows are some training results on classical control problems using our SoFM-PPO algorithm (an adaptation using Supervised Forward Model and Proximal Policy Optimization). Biological Context: \u00b6 The cerebellum have been long theorized to play an crucial rule in motor control and learning (Forward modeling). Corollary discharge encodes a efferent copy of the motor command to be processed to predict the consequences of actions before sensory feedback is available. Such process would help us predicts how the sensory state of our body will change and how should these actions be performed, achieving better performances in control. Using examples from (Albert and Shadmehr, 2018), with the starting and ending positions in hand, the parietal regions of your cerebral cortex compute the path of the arm that connects these positions in space the trajectory of the movement. After the trajectory is determined, your primary motor cortex and other associated pre-motor areas then carefully transform this sensory signal into a motor plan, namely the patterns of muscle contraction that will move your arm along the desired path towards the coffee. SFM & SoFM System Overview \u00b6 This is an overview of our model: Schematics for Forward Models Analogy \u00b6 Mountains After Mountains \u00b6 Every agent that learns in different world serves as a constraint for each other. They explore the world from their own perspective and \u201cpull\u201d each other on the way to prevent any one of them from falling into the \u201clocal optimality illusion\u201d that one sees in one moment. Wold Model \u00b6 We believe that policy is to a task specific, but if we build an reward model or an world model, such model may be agonist of the environment or the task, achieveing continual learning purpose. Constraint Solving \u00b6 Such a forward model can be interpreted in various ways from a constraint solving perspective: We can consider such model as a constrained process, forcing the agent to learn while incorporating its previous experiences. Similarly, the forward model's facilitation does not come directly from action facilitation but rather from providing a better representation of the feature space \\(\\vec \\phi(\\vec x)\\) that the agent has built up. It's about operating in this imaginary state . It is never about explicitly facilitating actions but rather implicitly making the model understand the dynamics and interactions in this space more comprehensively.","title":"Why SoFM?"},{"location":"#supervised-sub-optimality-forward-model","text":"I am currently researching into the theoretical aspects of Continual Learning and Transfer Learning with advising from professor Talmo Pereira in the Salk Institute. I try to frame the general problem of Continual Learning from the perspective of Reinforcement Learning & Cognitive Neuroscience, hoping to develop algorithms that utilize the same strategies of \"how we learn\" onto an artificial agent. Moreover, I hope that such development can serve more than an engineering improvmenet, but rather contributing back to the neuroscience community to understand more about ourselves. I am developing a conceptual framework that may be used for families of algorithms and I am trying to build internal representations for artificial agents through designing a Forward Model , one similar to what we think the Cerabellum is doing in human brain, so their learned skills in one task can be modularized and transferable to other control tasks. The belows are some training results on classical control problems using our SoFM-PPO algorithm (an adaptation using Supervised Forward Model and Proximal Policy Optimization).","title":"Supervised &amp; Sub-optimality Forward Model"},{"location":"#biological-context","text":"The cerebellum have been long theorized to play an crucial rule in motor control and learning (Forward modeling). Corollary discharge encodes a efferent copy of the motor command to be processed to predict the consequences of actions before sensory feedback is available. Such process would help us predicts how the sensory state of our body will change and how should these actions be performed, achieving better performances in control. Using examples from (Albert and Shadmehr, 2018), with the starting and ending positions in hand, the parietal regions of your cerebral cortex compute the path of the arm that connects these positions in space the trajectory of the movement. After the trajectory is determined, your primary motor cortex and other associated pre-motor areas then carefully transform this sensory signal into a motor plan, namely the patterns of muscle contraction that will move your arm along the desired path towards the coffee.","title":"Biological Context:"},{"location":"#sfm-sofm-system-overview","text":"This is an overview of our model: Schematics for Forward Models","title":"SFM &amp; SoFM System Overview"},{"location":"#analogy","text":"","title":"Analogy"},{"location":"progress/","text":"Question Trying to Solve \u00b6 Question 1: \u00b6 Does establishing a Forward Model (instance of a world model), similar to the Cerebellum's function, facilitate motor action execution by providing a motor plan derived from previous motor control experiences for additional guidance (compare to pure sensory feedback like in model-free RL)? Moreover, can this new motor learning process be incorporated into the GDP for future motor controls? Objective 1: See if such biologically inspired strategy (for example, maybe using mechanistic insight, maybe using neuronal representation as inductive biases) improves performance; Objective 2: See if the Forward Model would resemble functionality and behavior of the cerebellum (for example, showing gradual learning of new motor skills). Idealy using a more biological realistic model with more biological realistic task such as the rodent model in VNL. Question 2: \u00b6 With just change of the understanding for the rules of the world (intention changes), can I still find a sub-optimal point in this training world such that it works still fine or even better than solely one-world-model trained agent in the other world? SFM-PPO & SoFM-PPO Control Examples \u00b6 SFM-PPO is a vriant of PPO where an Supervised Forward Model (SFM) is added to understand the dynamics of teh environment amd SoFM-PPO is a variant of PPO (Sub-Optimal Forward Model) as well where instead of just the SFM, the model is also trained to find \"local minimum\" or sub-optimal minimum in one task in order for completions in more than just one task. The below are a few Deep-RL Half Cheetah agent demos: Your browser does not support the video tag. Trained using PPO on \"Jump Task\" and stuck on weird local minimum Your browser does not support the video tag. Trained using SFM-PPO on \"Normal Running Task\" Your browser does not support the video tag. Trained using SoFM-PPO on \"Normal Running Task\" but with world model intention bounding Latent Created Example \u00b6 Demonstartion of latent representation of agent during task Your browser does not support the video tag. Deep-RL SoFM-PPO Half Cheetah agent trained and visualize with PCA Performance in Sensory Delayed Environment \u00b6 Simple testing in mimicing delayed sensory environment like in real life: Deep-RL SoFM-PPO Half Cheetah agent evaluated on sensory delayed environment","title":"Progress"},{"location":"progress/#question-trying-to-solve","text":"","title":"Question Trying to Solve"},{"location":"progress/#sfm-ppo-sofm-ppo-control-examples","text":"SFM-PPO is a vriant of PPO where an Supervised Forward Model (SFM) is added to understand the dynamics of teh environment amd SoFM-PPO is a variant of PPO (Sub-Optimal Forward Model) as well where instead of just the SFM, the model is also trained to find \"local minimum\" or sub-optimal minimum in one task in order for completions in more than just one task. The below are a few Deep-RL Half Cheetah agent demos: Your browser does not support the video tag. Trained using PPO on \"Jump Task\" and stuck on weird local minimum Your browser does not support the video tag. Trained using SFM-PPO on \"Normal Running Task\" Your browser does not support the video tag. Trained using SoFM-PPO on \"Normal Running Task\" but with world model intention bounding","title":"SFM-PPO &amp; SoFM-PPO Control Examples"},{"location":"progress/#latent-created-example","text":"Demonstartion of latent representation of agent during task Your browser does not support the video tag. Deep-RL SoFM-PPO Half Cheetah agent trained and visualize with PCA","title":"Latent Created Example"},{"location":"progress/#performance-in-sensory-delayed-environment","text":"Simple testing in mimicing delayed sensory environment like in real life: Deep-RL SoFM-PPO Half Cheetah agent evaluated on sensory delayed environment","title":"Performance in Sensory Delayed Environment"}]}