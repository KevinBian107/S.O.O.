{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, MSE: 1.9313436077706032\n",
      "Epoch 2, MSE: 0.4142763547264412\n",
      "Epoch 3, MSE: 0.09726824222930197\n",
      "Epoch 4, MSE: 0.027565145440599265\n",
      "Epoch 5, MSE: 0.01194459224828502\n",
      "Epoch 6, MSE: 0.00842832262960437\n",
      "Epoch 7, MSE: 0.007639625227691973\n",
      "Epoch 8, MSE: 0.007464822237237552\n",
      "Epoch 9, MSE: 0.007427125945262293\n",
      "Epoch 10, MSE: 0.007419503375618475\n",
      "Epoch 11, MSE: 0.007418214016196329\n",
      "Epoch 12, MSE: 0.007418129917205473\n",
      "Epoch 13, MSE: 0.00741820971745897\n",
      "Epoch 14, MSE: 0.007418274799060512\n",
      "Epoch 15, MSE: 0.0074183119066360695\n",
      "Epoch 16, MSE: 0.007418330959327467\n",
      "Epoch 17, MSE: 0.007418340345737021\n",
      "Epoch 18, MSE: 0.007418344887452496\n",
      "Epoch 19, MSE: 0.00741834706714651\n",
      "Epoch 20, MSE: 0.007418348109325135\n",
      "Epoch 21, MSE: 0.007418348606763008\n",
      "Epoch 22, MSE: 0.007418348844005345\n",
      "Epoch 23, MSE: 0.00741834895711252\n",
      "Epoch 24, MSE: 0.007418349011028699\n",
      "Epoch 25, MSE: 0.007418349036727826\n",
      "Epoch 26, MSE: 0.007418349048976945\n",
      "Epoch 27, MSE: 0.0074183490548152585\n",
      "Epoch 28, MSE: 0.007418349057597964\n",
      "Epoch 29, MSE: 0.00741834905892429\n",
      "Epoch 30, MSE: 0.007418349059556459\n",
      "Epoch 31, MSE: 0.0074183490598577675\n",
      "Epoch 32, MSE: 0.007418349060001379\n",
      "Epoch 33, MSE: 0.007418349060069832\n",
      "Epoch 34, MSE: 0.007418349060102459\n",
      "Epoch 35, MSE: 0.007418349060117999\n",
      "Epoch 36, MSE: 0.007418349060125415\n",
      "Epoch 37, MSE: 0.007418349060128951\n",
      "Epoch 38, MSE: 0.007418349060130642\n",
      "Epoch 39, MSE: 0.00741834906013143\n",
      "Epoch 40, MSE: 0.007418349060131822\n",
      "Epoch 41, MSE: 0.007418349060132002\n",
      "Epoch 42, MSE: 0.007418349060132093\n",
      "Epoch 43, MSE: 0.007418349060132131\n",
      "Epoch 44, MSE: 0.007418349060132156\n",
      "Epoch 45, MSE: 0.007418349060132153\n",
      "Epoch 46, MSE: 0.007418349060132167\n",
      "Epoch 47, MSE: 0.007418349060132163\n",
      "Epoch 48, MSE: 0.007418349060132164\n",
      "Epoch 49, MSE: 0.007418349060132164\n",
      "Epoch 50, MSE: 0.007418349060132171\n",
      "Epoch 51, MSE: 0.007418349060132169\n",
      "Epoch 52, MSE: 0.007418349060132163\n",
      "Epoch 53, MSE: 0.007418349060132173\n",
      "Epoch 54, MSE: 0.007418349060132173\n",
      "Epoch 55, MSE: 0.007418349060132173\n",
      "Epoch 56, MSE: 0.007418349060132173\n",
      "Epoch 57, MSE: 0.007418349060132173\n",
      "Epoch 58, MSE: 0.007418349060132173\n",
      "Epoch 59, MSE: 0.007418349060132173\n",
      "Epoch 60, MSE: 0.007418349060132173\n",
      "Epoch 61, MSE: 0.007418349060132173\n",
      "Epoch 62, MSE: 0.007418349060132173\n",
      "Epoch 63, MSE: 0.007418349060132173\n",
      "Epoch 64, MSE: 0.007418349060132173\n",
      "Epoch 65, MSE: 0.007418349060132173\n",
      "Epoch 66, MSE: 0.007418349060132173\n",
      "Epoch 67, MSE: 0.007418349060132173\n",
      "Epoch 68, MSE: 0.007418349060132173\n",
      "Epoch 69, MSE: 0.007418349060132173\n",
      "Epoch 70, MSE: 0.007418349060132173\n",
      "Epoch 71, MSE: 0.007418349060132173\n",
      "Epoch 72, MSE: 0.007418349060132173\n",
      "Epoch 73, MSE: 0.007418349060132173\n",
      "Epoch 74, MSE: 0.007418349060132173\n",
      "Epoch 75, MSE: 0.007418349060132173\n",
      "Epoch 76, MSE: 0.007418349060132173\n",
      "Epoch 77, MSE: 0.007418349060132173\n",
      "Epoch 78, MSE: 0.007418349060132173\n",
      "Epoch 79, MSE: 0.007418349060132173\n",
      "Epoch 80, MSE: 0.007418349060132173\n",
      "Epoch 81, MSE: 0.007418349060132173\n",
      "Epoch 82, MSE: 0.007418349060132173\n",
      "Epoch 83, MSE: 0.007418349060132173\n",
      "Epoch 84, MSE: 0.007418349060132173\n",
      "Epoch 85, MSE: 0.007418349060132173\n",
      "Epoch 86, MSE: 0.007418349060132173\n",
      "Epoch 87, MSE: 0.007418349060132173\n",
      "Epoch 88, MSE: 0.007418349060132173\n",
      "Epoch 89, MSE: 0.007418349060132173\n",
      "Epoch 90, MSE: 0.007418349060132173\n",
      "Epoch 91, MSE: 0.007418349060132173\n",
      "Epoch 92, MSE: 0.007418349060132173\n",
      "Epoch 93, MSE: 0.007418349060132173\n",
      "Epoch 94, MSE: 0.007418349060132173\n",
      "Epoch 95, MSE: 0.007418349060132173\n",
      "Epoch 96, MSE: 0.007418349060132173\n",
      "Epoch 97, MSE: 0.007418349060132173\n",
      "Epoch 98, MSE: 0.007418349060132173\n",
      "Epoch 99, MSE: 0.007418349060132173\n",
      "Epoch 100, MSE: 0.007418349060132173\n",
      "Final weights: [[2.01328517]]\n",
      "Final bias: [0.50161319]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 100  # Number of times to iterate over the dataset\n",
    "learning_rate = 0.01  # Learning rate\n",
    "num_data = 50  # Number of data points\n",
    "\n",
    "# Data generation: Simple linear relation with some noise\n",
    "np.random.seed(42)  # For reproducibility\n",
    "inputs = np.random.randn(num_data, 1)  # Random inputs\n",
    "weights_true = np.array([[2.0]])  # True weights\n",
    "bias_true = np.array([0.5])  # True bias\n",
    "outputs = inputs.dot(weights_true) + bias_true + 0.1 * np.random.randn(num_data, 1)  # Outputs with noise\n",
    "\n",
    "# Initial random weights and bias\n",
    "weights = np.random.randn(1, 1)\n",
    "bias = np.random.randn(1)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "# Online gradient descent algorithm\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_data):\n",
    "        input = inputs[i:i+1]\n",
    "        output = outputs[i:i+1]\n",
    "        \n",
    "        # Prediction\n",
    "        prediction = input.dot(weights) + bias\n",
    "        \n",
    "        # Error calculation\n",
    "        error = output - prediction\n",
    "        \n",
    "        # Gradient calculation\n",
    "        weights_gradient = -2 * input.T.dot(error)\n",
    "        bias_gradient = -2 * error.sum()\n",
    "        \n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * weights_gradient\n",
    "        bias -= learning_rate * bias_gradient\n",
    "        \n",
    "    # Print the mean squared error at each epoch\n",
    "    prediction = inputs.dot(weights) + bias\n",
    "    mse = mean_squared_error(outputs, prediction)\n",
    "    print(f\"Epoch {epoch+1}, MSE: {mse}\")\n",
    "\n",
    "print(\"Final weights:\", weights)\n",
    "print(\"Final bias:\", bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
